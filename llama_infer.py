import sys
import argparse
import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from LLMPruner.peft import PeftModel

#from utils.callbacks import Iteratorize, Stream
#from utils.prompter import Prompter

if torch.cuda.is_available():
    device = "cuda"
else:
    device = "cpu"
torch_version = int(torch.__version__.split('.')[1])
import util


def main(args):
    model_path = ""
    if args.model_type == 'pretrain':
        if args.base_model=="decapoda-research/llama-7b-hf":
            from transformers import LlamaTokenizer
            tokenizer = LlamaTokenizer.from_pretrained(args.base_model)
        else:
            tokenizer = AutoTokenizer.from_pretrained(args.base_model)
        model = AutoModelForCausalLM.from_pretrained(
            args.base_model,
            low_cpu_mem_usage=True if torch_version >=9 else False
        )
        description = "Model Type: {}\n Base Model: {}".format(args.model_type, args.base_model)
        model_path = args.base_model
    elif args.model_type == 'pruneLLM':
        pruned_dict = torch.load(args.ckpt, map_location='cpu')
        tokenizer, model = pruned_dict['tokenizer'], pruned_dict['model']
        description = "Model Type: {}\n Pruned Model: {}".format(args.model_type, args.ckpt)
        model_path = args.ckpt
    else:
        raise NotImplementedError

    if device == "cuda":
        model.half()
        model = model.cuda()
    
    # unwind broken decapoda-research config
    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk
    model.config.bos_token_id = 1
    model.config.eos_token_id = 2

    model.eval()

    util.model_info(model=model)

    def func():
        input = """Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.
Daniel: Hello, Girafatron!
Girafatron:
""" 
        inputs = tokenizer(input, return_tensors="pt")
        input_ids = inputs["input_ids"].to(device)
        with torch.no_grad():
            generation_output = model.generate(
                input_ids=input_ids,
                do_sample=True,
                top_k=50,
                top_p=0.75,
                temperature=0.1,
                max_length=128,
                return_dict_in_generate=True,
            )
        s = generation_output.sequences[0]
        output = tokenizer.decode(s)
        print(f"Result:\n{output}")
    
    ts = []
    import time
    for i in range(100):
        print("="*100)
        print(i)
        start = time.time()
        func()
        ts.append(time.time()-start)
        print(f"{i}: {ts[-1]}")
        
    print(model_path, sum(ts)/len(ts), ts)


if __name__ == "__main__":
    # sys.argv.extend("--model_type pretrain".split())
    sys.argv.extend("--model_type pruneLLM --ckpt prune_log/llama_prune/pytorch_model.bin".split())
    parser = argparse.ArgumentParser(description='Tuning Pruned LLaMA (huggingface version)')

    parser.add_argument('--base_model', type=str, default="decapoda-research/llama-7b-hf", help='base model name')
    parser.add_argument('--model_type', type=str, required=True, help = 'choose from ')
    parser.add_argument('--ckpt', type=str, default=None)
    parser.add_argument('--lora_ckpt', type=str, default=None)
    parser.add_argument('--share_gradio', action='store_true')

    args = parser.parse_args()
    main(args)

# 12836MB prune_log/llama_prune/pytorch_model.bin 2.079325330257416 [4.0586793422698975, 2.0269763469696045, 2.053722381591797, 2.053236484527588, 2.0288567543029785, 2.0252625942230225, 2.0172524452209473, 2.0158989429473877, 2.0407168865203857, 2.0586140155792236, 2.061347484588623, 2.068382740020752, 2.0456371307373047, 2.0580577850341797, 2.091254234313965, 2.0448336601257324, 2.015259265899658, 2.0283071994781494, 2.0356264114379883, 2.001568555831909, 2.0030624866485596, 1.987748146057129, 2.008700132369995, 1.9924473762512207, 1.9726223945617676, 2.0028915405273438, 1.990344524383545, 2.021895408630371, 1.9874134063720703, 2.0017974376678467, 2.0570693016052246, 1.9891793727874756, 1.998636245727539, 2.0148544311523438, 1.998157024383545, 2.010836362838745, 1.9912097454071045, 1.9954373836517334, 2.001116991043091, 1.9909377098083496, 1.9903864860534668, 1.962317943572998, 1.9853501319885254, 2.009613037109375, 2.2221128940582275, 2.0251331329345703, 1.992582082748413, 2.018861770629883, 2.031585454940796, 2.0062758922576904, 2.2681398391723633, 2.247082471847534, 2.181591272354126, 1.9971692562103271, 2.1454427242279053, 1.9898591041564941, 2.004059076309204, 1.9827957153320312, 2.010251045227051, 2.0125932693481445, 1.9994134902954102, 1.9969596862792969, 2.064206600189209, 2.0913703441619873, 2.0398008823394775, 2.0226263999938965, 2.241724729537964, 2.1391525268554688, 2.0481927394866943, 2.01926326751709, 2.005680799484253, 2.012341022491455, 2.0224640369415283, 2.0400190353393555, 2.0053324699401855, 2.059840202331543, 2.0276365280151367, 2.034632444381714, 1.9926066398620605, 2.0023210048675537, 1.9819235801696777, 1.9824204444885254, 1.9871149063110352, 2.0029754638671875, 2.2296335697174072, 2.254627227783203, 2.2038800716400146, 2.1844775676727295, 2.143862724304199, 2.173031806945801, 2.122483730316162, 2.1950457096099854, 2.215975046157837, 2.2299723625183105, 2.223062753677368, 2.2042479515075684, 2.1909263134002686, 2.23514461517334, 2.1952788829803467, 2.1818127632141113]
# prune_log/llama_prune/pytorch_model.bin 2.195541987419128 [3.8939120769500732, 1.957700490951538, 2.123213529586792, 2.2158236503601074, 2.2124767303466797, 2.1538639068603516, 2.080726146697998, 1.9505493640899658, 1.975243091583252, 2.124591112136841, 1.9117090702056885, 1.9261581897735596, 2.129422187805176, 2.1612250804901123, 2.150597333908081, 2.1209871768951416, 2.181398630142212, 2.174907922744751, 2.152116060256958, 2.194139003753662, 2.1603593826293945, 2.168696880340576, 2.1758673191070557, 2.1819663047790527, 2.15433406829834, 2.140023946762085, 2.1483516693115234, 2.1580634117126465, 2.1908600330352783, 2.170606851577759, 2.179886817932129, 2.20509672164917, 2.1610331535339355, 2.2191452980041504, 2.166861057281494, 2.139564037322998, 2.1473989486694336, 2.136765718460083, 2.148545026779175, 2.148606777191162, 2.150665283203125, 2.180375099182129, 2.1800713539123535, 2.1422157287597656, 2.1757307052612305, 2.227367877960205, 2.2200355529785156, 2.1569347381591797, 2.128359079360962, 2.1932103633880615, 2.211839199066162, 2.195828676223755, 2.1614186763763428, 2.220761299133301, 2.195004940032959, 2.1993091106414795, 2.261321544647217, 2.248724937438965, 2.235452651977539, 2.1900088787078857, 2.22064471244812, 2.2615394592285156, 2.2176947593688965, 2.2517123222351074, 2.2331807613372803, 2.2299585342407227, 2.2081923484802246, 2.2603917121887207, 2.179459810256958, 2.2413370609283447, 2.2110698223114014, 2.2173149585723877, 2.2190542221069336, 2.2442803382873535, 2.21127986907959, 2.2309329509735107, 2.216181516647339, 2.1991512775421143, 2.2267816066741943, 2.1922686100006104, 2.2231290340423584, 2.2288355827331543, 2.2129454612731934, 2.2300350666046143, 2.1723554134368896, 2.211841106414795, 2.1861283779144287, 2.218308925628662, 2.1894099712371826, 2.194361686706543, 2.1933813095092773, 2.167325019836426, 2.170682668685913, 2.164052963256836, 2.211421489715576, 2.212071418762207, 2.224285840988159, 2.235532283782959, 2.2375712394714355, 2.2306673526763916]

# 15416MB decapoda-research/llama-7b-hf 1.8810337233543395 [3.6077468395233154, 1.7005205154418945, 1.7005484104156494, 1.6802563667297363, 1.6486949920654297, 1.6689331531524658, 1.6911401748657227, 1.6896352767944336, 1.6884098052978516, 1.672811508178711, 1.6693782806396484, 1.6859917640686035, 1.6867499351501465, 1.692028522491455, 1.7109625339508057, 1.7064709663391113, 1.7119474411010742, 1.7111830711364746, 1.735628604888916, 1.7021596431732178, 1.7178828716278076, 1.828765869140625, 1.9286086559295654, 1.8978431224822998, 1.7352182865142822, 1.7466843128204346, 1.7302045822143555, 1.7331082820892334, 1.929565668106079, 1.9126429557800293, 1.9533202648162842, 1.9263548851013184, 1.925722360610962, 1.9148926734924316, 1.908815860748291, 1.9327611923217773, 1.8870816230773926, 1.9028620719909668, 1.9166789054870605, 1.9245495796203613, 1.9320127964019775, 1.9105339050292969, 1.8933074474334717, 1.9042646884918213, 1.915264368057251, 1.9144492149353027, 1.9208760261535645, 1.9030914306640625, 1.901200294494629, 1.9278409481048584, 2.00521183013916, 1.934366226196289, 1.945850133895874, 1.8940293788909912, 1.6889219284057617, 1.6421668529510498, 1.650085210800171, 1.6635832786560059, 1.9045002460479736, 1.9105117321014404, 1.9387741088867188, 1.9606051445007324, 1.9443747997283936, 1.9210891723632812, 1.9159331321716309, 1.928828239440918, 2.060483694076538, 2.0284416675567627, 1.9726717472076416, 1.9441535472869873, 1.9510419368743896, 1.9630515575408936, 1.9483551979064941, 1.9355318546295166, 1.9315319061279297, 1.9373526573181152, 1.9279487133026123, 1.9232544898986816, 1.918743371963501, 1.912046194076538, 1.9703352451324463, 1.9280014038085938, 1.9702677726745605, 1.9487595558166504, 1.908750057220459, 1.9188671112060547, 1.9509291648864746, 1.9392776489257812, 1.9458847045898438, 1.913273811340332, 1.9038896560668945, 1.913802146911621, 1.9260008335113525, 1.9452455043792725, 1.9275062084197998, 1.9258294105529785, 1.932410478591919, 1.9363043308258057, 1.9268507957458496, 1.9301435947418213]
# decapoda-research/llama-7b-hf 1.8710266089439391 [3.8045947551727295, 1.937943458557129, 1.913022756576538, 1.922875165939331, 1.9002141952514648, 1.893855333328247, 1.901754379272461, 1.9112560749053955, 1.9039192199707031, 1.903198003768921, 1.8987035751342773, 1.9194715023040771, 1.911301612854004, 1.9006857872009277, 1.8918166160583496, 1.9006774425506592, 1.8968393802642822, 1.8935859203338623, 1.9018049240112305, 1.903273105621338, 1.877967119216919, 1.8689913749694824, 1.892719030380249, 1.8908488750457764, 1.8801417350769043, 1.8898024559020996, 1.909956693649292, 1.882338047027588, 1.9054145812988281, 1.8888683319091797, 1.895747423171997, 1.8857381343841553, 1.8906714916229248, 1.90175199508667, 1.8753085136413574, 1.9169392585754395, 1.8970165252685547, 1.8987765312194824, 1.8758392333984375, 1.8948006629943848, 1.904891014099121, 1.9055228233337402, 1.9038176536560059, 1.8962903022766113, 1.88446044921875, 1.9001052379608154, 1.8859052658081055, 1.9081950187683105, 1.9107413291931152, 1.7390198707580566, 1.9181923866271973, 1.8926444053649902, 1.8961560726165771, 1.9205963611602783, 1.9095113277435303, 1.9604456424713135, 1.9399783611297607, 1.9607021808624268, 1.9506680965423584, 1.918426513671875, 1.9166738986968994, 1.9502811431884766, 1.9433174133300781, 1.9412884712219238, 1.9023056030273438, 1.917064905166626, 1.9173648357391357, 1.655733346939087, 1.7770936489105225, 1.7032208442687988, 1.7299070358276367, 1.690462350845337, 1.6913461685180664, 1.7281498908996582, 1.8995418548583984, 1.7464635372161865, 1.727269172668457, 1.7226462364196777, 1.7222521305084229, 1.7350084781646729, 1.747570276260376, 1.7267346382141113, 1.7204878330230713, 1.7340855598449707, 1.7138667106628418, 1.8418047428131104, 1.8560514450073242, 1.7162706851959229, 1.7085626125335693, 1.7073719501495361, 1.697810173034668, 1.705556869506836, 1.7105381488800049, 1.8012542724609375, 1.8704991340637207, 1.9041814804077148, 1.8854572772979736, 1.7270030975341797, 1.688180685043335, 1.6852807998657227]
